{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Import relevant packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom collections import Counter\n\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport matplotlib.pyplot as plt\nimport methods_py as m\nimport re\nimport sys,time\n\n# reload to make sure we have the latest version of method.py loaded\nimport importlib\nimportlib.reload(m)\n\n# NLTK package for stopwords\nimport nltk\nnltk.download(['punkt', 'wordnet','stopwords'])\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n\n# sklearn \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report, plot_precision_recall_curve, roc_auc_score\n\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.naive_bayes import MultinomialNB","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"References Credits: https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Read in the train data \ndf = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip')\n# display the first 5 rows\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 1. Explore the dataframe","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# call custom method to explore the dataframe\nm.explore_df(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# list of columns for the labelled comment type \ndf.drop(['id','comment_text'],axis=1).columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# adding some of the ad-hoc words coming up as frequent and I want to exclude those\nstop_words.update([\"u\",\"go\",\"as\",\"like\",\"wikipedia\",\"jim\",\"hi\",\"get\"])\n\n# tokeinze method to remove punctuations, lemmatize and remove stop words\ndef tokenize(text):\n    '''\n    INPUT\n    text- the text that needs to be tokenized\n    \n    OUTPUT\n    tokens - a list of tokenized words after cleaning up the input text\n    \n    This function :\n    1. converts text to all lower case and removes punctuations\n    2. tokenize entire text into words\n    2. lemmatize each word using WordNetLemmatizer\n    3. remove all stop words from text as per english corpus of stop words\n    '''\n    # normalize case and remove punctuation\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n    \n    # tokenize text\n    tokens = word_tokenize(text)\n    # initiate lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    # lemmatize and remove stop words\n    tokens = [lemmatizer.lemmatize(word) for word in tokens if not word in stop_words]\n    \n    return tokens","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\" ###### WARNING : Comments below contain strong profanity and abusive words ####### \\n\")\n\n# utilizing counter to get the most common 10 words coming up in each category\nfor category in list(df.drop(['id','comment_text'],axis=1).columns):\n    print(\"10 most frequent words in \"+category+\" comments\")\n    print(dict(Counter(tokenize(\" \".join(df[df[category]==1].comment_text))).most_common(10)).keys())\n    print(\"\\n ********************* \\n\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# access custom method to explore dataframe\nm.explore_null(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Lets explore the number of comments for each category\n# This gives us a quick view of how the dataset is populated\n\ndf_toxic = df.drop(['id', 'comment_text'], axis=1)\ncounts = []\ncategories = list(df_toxic.columns.values)\nfor i in categories:\n    counts.append((i, df_toxic[i].sum()))\ndf_stats = pd.DataFrame(counts, columns=['category', 'number_of_comments'])\n\ndf_stats.plot(x='category', y='number_of_comments', kind='barh', legend=False, grid=True, figsize=(8, 5))\nplt.title(\"Number of comments per category\")\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('category', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# How many comments have multi labels?\n\ndf.drop(['id','comment_text'],axis=1).sum(axis=1).value_counts().plot(kind='bar')\nplt.ylabel('# of Occurrences', fontsize=12)\nplt.xlabel('# of categories', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# % of comments which are labeled\n\nround((sum(df.drop(['id','comment_text'],axis=1).sum(axis=1) > 0)/len(df))*100,2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation : This is an imbalanced set with almost 90% of comments not classified into any one of the 6 categories","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#The distribution of the number of words in comment texts.\n\nlens = df.comment_text.str.len()\nlens.hist(bins = np.arange(0,5000,50))\n\nplt.ylabel('# of occurences', fontsize=12)\nplt.xlabel('# of words', fontsize=12)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Modeling\n\n#### We evaluate four different classification models : LinearSVC, NaiveBayes, Decision Tree Classifer and Logistic Regression ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Credits: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n#          https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n#          https://scikit-learn.org/stable/modules/multiclass.html\n        \ndef build_model(model_type, c):\n    '''\n    INPUT\n    model type - which model we want to build between Linear SVC, NaiveBayes and Logistic Regression\n    \n    OUTPUT\n    pipeline- a pipeline built to vectorize, tokenize, transform and classify text data\n    \n    This function :\n    1. builds a pipeline of countvectorizer, tfidf transformer, and\n    2. a random forest multi output classifier\n    '''\n    #check the model_type provided by user\n    if model_type == 'linear_svc':\n        pipeline = Pipeline([\n    ('vect', CountVectorizer(tokenizer=tokenize)),\n    ('tfidf', TfidfTransformer()),\n    ('clf', OneVsRestClassifier(LinearSVC(C=c), n_jobs=3))\n    ])\n        \n    #check the model_type provided by user    \n    elif model_type== 'naivebayes':\n        pipeline = Pipeline([\n    ('vect', CountVectorizer(tokenizer=tokenize)),\n    ('tfidf', TfidfTransformer()),\n    ('clf', OneVsRestClassifier(MultinomialNB(fit_prior=True, class_prior=None)))\n    ])\n        \n    #check the model_type provided by user    \n    elif model_type== 'logistic_reg':\n        pipeline = Pipeline([\n    ('vect', CountVectorizer(tokenizer=tokenize)),\n    ('tfidf', TfidfTransformer()),\n    ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1))\n    ])\n        \n    #check the model_type provided by user    \n    elif model_type== 'random_forest':\n        pipeline = Pipeline([\n    ('vect', CountVectorizer(tokenizer=tokenize)),\n    ('tfidf', TfidfTransformer()),\n    ('clf', OneVsRestClassifier(RandomForestClassifier(max_depth=2, random_state=0), n_jobs=1))\n    ])\n        \n    #check the model_type provided by user    \n    elif model_type== 'decision_tree':\n        pipeline = Pipeline([\n    ('vect', CountVectorizer(tokenizer=tokenize)),\n    ('tfidf', TfidfTransformer()),\n    ('clf', OneVsRestClassifier(DecisionTreeClassifier(random_state=0), n_jobs=1))\n    ])\n    \n    # return the built pipeline\n    return pipeline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 3. Predict and evaluate models","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Credits: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-imbalanced-classification/\n\nWe consider using the ROC AUC score for evaluation the model as it is an imbalanced dataset with around 90% of comments with no labels i.e. clean comments\n\n***Although widely used, the ROC AUC is not without problems.***\n\nFor imbalanced classification with a severe skew and few examples of the minority class, the ROC AUC can be misleading. This is because a small number of correct or incorrect predictions can result in a large change in the ROC Curve or ROC AUC score.\n\nA common alternative is the precision-recall curve and area under curve (Precision-Recall Curves and AUC)\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate_model(model, X_test, Y_test):\n    '''\n    INPUT\n    model : the model that needs to be evaluated\n    X_test : validation data set i.e. messages in this case\n    Y_test : the output data for X_test validation set i.e. 36 categories values\n    category_names : the 36 category names\n    \n    OUTPUT\n    classification report for the model based on predictions, gives\n    the recall, precision and f1 score\n    \n    This function :\n    1. utilizes the input model to make predictions\n    2. compares the predictions to the test data to provide a classification report\n    '''\n    \n    y_pred = model.predict(X_test)\n    \n    # ROC AUC Score\n    return round(roc_auc_score(Y_test, y_pred),3)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n\n# we will be using each one of the category below to utilize them as the Predicted value (Y)\ncategories = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\n# we will utilize the comment text column to predict Y\nX= df['comment_text']\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We first try working with a Naive Bayes Model to baseline our score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_type = 'naivebayes'\n\nprint(\"Training with a \"+model_type+\" model\")\n\nmodel_dict_nb= dict()\n\nfor category in categories:\n\n    print(\" ---------------------------------------- \")\n    #print(\" Working on evaluating ROC AUC score for \"+category+\" comments ............\")\n    \n    # the category becomes our predicted variable\n    Y= df[category]\n    \n    # split the data set to a train and test set\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n      \n    print('Building model for ',category,' comments')\n    model = build_model(model_type,1)\n\n    print('Training model...')\n    start_time = time.time()\n    model.fit(X_train, Y_train)\n    elapsed_time = round((time.time() - start_time)/60 , 2)\n    print('Training time...{} minutes'.format(elapsed_time))\n\n    print('Evaluating model...')\n    print(\"ROC AUC Score: \",evaluate_model(model, X_test, Y_test))\n            \n    #save the model into a dictionary\n    model_dict_nb[model] = category\n    print(\"Model saved for \",category,\" comments\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Logistic model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_type = 'logistic_reg'\n\nprint(\"Training with a \"+model_type+\" model\")\n\nmodel_dict_lr= dict()\n\nfor category in categories:\n\n    print(\" ---------------------------------------- \")\n    #print(\" Working on evaluating ROC AUC score for \"+category+\" comments ............\")\n    \n    # the category becomes our predicted variable\n    Y= df[category]\n    \n    # split the data set to a train and test set\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n      \n    print('Building model for ',category,' comments')\n    model = build_model(model_type,1)\n\n    print('Training model...')\n    start_time = time.time()\n    model.fit(X_train, Y_train)\n    elapsed_time = round((time.time() - start_time)/60 , 2)\n    print('Training time...{} minutes'.format(elapsed_time))\n\n    print('Evaluating model...')\n    print(\"ROC AUC Score: \",evaluate_model(model, X_test, Y_test))\n            \n    #save the model into a dictionary\n    model_dict_lr[model] = category\n    print(\"Model saved for \",category,\" comments\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets see if a decision tree classifier helps us get a better score","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_type = 'decision_tree'\n\nprint(\"Training with a \"+model_type+\" model\")\n\nmodel_dict_dt= dict()\n\nfor category in categories:\n\n    print(\" ---------------------------------------- \")\n    #print(\" Working on evaluating ROC AUC score for \"+category+\" comments ............\")\n    \n    # the category becomes our predicted variable\n    Y= df[category]\n    \n    # split the data set to a train and test set\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n      \n    print('Building model for ',category,' comments')\n    model = build_model(model_type,1)\n\n    print('Training model...')\n    start_time = time.time()\n    model.fit(X_train, Y_train)\n    elapsed_time = round((time.time() - start_time)/60 , 2)\n    print('Training time...{} minutes'.format(elapsed_time))\n\n    print('Evaluating model...')\n    print(\"ROC AUC Score: \",evaluate_model(model, X_test, Y_test))\n            \n    #save the model into a dictionary\n    model_dict_dt[model] = category\n    print(\"Model saved for \",category,\" comments\")\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Model Observation\nVery interesting to see that for toxic comments, \nthe Linear SVC is able to generate ROC AUC Score of 0.843 in 2.45 minutes  \nvs \nDecision tree classifier generating ROC AUC Score of 0.835 in 13.6 minutes\n\nIMPORTANT NOTE: More complex algorithms wont necessarily end up with same efficiency and may be even lesser score\n\nNEXT STEP: \n1. Why is Linear SVC performing better score and faster than Decision tree ?\n2. Why does the Random forest classifier does worse than Decision tree?\n3. Why logistic regression and NaiveBayes are doing worse than Linear SVC\n4. Are these specific to text classification?","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Linear SVC Model","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Credits: https://medium.com/all-things-ai/in-depth-parameter-tuning-for-svc-758215394769","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model_type = 'linear_svc'\n\nprint(\"Training with a \"+model_type+\" model\")\n\nmodel_dict_svc= dict()\n\nfor category in categories:\n\n    print(\" ---------------------------------------- \")\n    #print(\" Working on evaluating ROC AUC score for \"+category+\" comments ............\")\n    \n    # the category becomes our predicted variable\n    Y= df[category]\n    \n    # split the data set to a train and test set\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n    \n    tmp_score = 0\n    \n    for C in [0.1,1,10]:\n        \n        print('Building model for ',category,' comments for C=',C,'...')\n        model = build_model(model_type,C)\n        \n        print('Training model...')\n        start_time = time.time()\n        model.fit(X_train, Y_train)\n        elapsed_time = round((time.time() - start_time)/60 , 2)\n        print('Training time...{} minutes'.format(elapsed_time))\n\n        print('Evaluating model...')\n        print(\"ROC AUC Score: \",evaluate_model(model, X_test, Y_test))\n        \n        if evaluate_model(model, X_test, Y_test) > tmp_score:\n            \n            #assign the latest evaluation score to tmp_score\n            tmp_score = evaluate_model(model, X_test, Y_test)\n            #assign this model to the tmp_model\n            tmp_model = model\n            \n    #save the model into a dictionary\n    model_dict_svc[tmp_model] = category\n    print(\"Model saved for \",category,\" comments with the highest score of \",tmp_score)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on our comparisons between NaiveBayes , Logistic Regression, Decision Tree classifier and Linear SVC Model \n\nwe consider utilizing the Linear SVC Model for our classification\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Comparing classifiers : https://www.cnblogs.com/yymn/p/4518016.html","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"## Testing some unseen user provided input\n\n#### ---- User Warning: The comments below contain profanity and abusive language ------","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"comment_list= \\\n[\"==shame on you all!!!== \\n\\n You want to speak about gays and not about romanians...\",\"what the hell is wrong with u\",\"I love being here\",\"I am going to kill this bastard\",\n \"fuck this entire post\",\"you are an ignoratn bastard\",\"are you mad\",\"you gay idiot\",\n \"nigger go die somehere\",\"get your fagget ass out\",\"you are such a weiner pussy\",\"like the new ideas\",\"good things are done\"]\n\n\nfor comment in comment_list:\n    # new list\n    classification_list= list()\n    # switch to signify clean vs toxic comments\n    switch=1\n    # put the comment into a list form\n    inp = [comment]\n    # print the comment\n    print(\"\\n User Comment : \",comment,\"\\n\")\n    \n    for model in model_dict_svc.keys():    \n        if model.predict(inp):\n            classification_list.append(model_dict_svc[model])\n            switch=0\n    \n    if switch:\n        print(\"CLASSIFICATION: The comment is clean and does not contain any toxicity\")\n    else:\n        print(\"CLASSIFICATION: \", classification_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#set option to view the entire comment\npd.set_option('display.max_colwidth',None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Read in the train data \ndf_test = pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')\n#first 5 rows\ndf_test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to measure the time elapsed for producing the classification\nstart_time = time.time()\n\ndef classify_toxicity(text):\n    \n    class_list = list() \n    for model in model_dict_svc.keys():    \n        if model.predict([text]):\n            class_list.append(model_dict_svc[model])\n           \n    return class_list\n\ndf_test['classification'] = df_test['comment_text'].apply(lambda x: classify_toxicity(x))\n\nelapsed_time = round((time.time() - start_time)/60 , 2)\nprint('Time taken for Classification ...{} minutes'.format(elapsed_time))\n\n#final classification stored in a \"classification\" column with all labels applicable\ndf_test.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Conclusion","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"**Overall,the LinearSVC model is able to perform most efficiently in terms of training time and the ROC AUC Score.** \n\nOne of the reasons LinearSVC works better than Logistic Regression because LinearSVC tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data, while logistic regression does not, instead it can have different decision boundaries with different weights that are near the optimal point.\n\nSVM works well with unstructured and semi-structured data like text and images while logistic regression works with already identified independent variables. SVM is based on geometrical properties of the data while logistic regression is based on statistical approaches","execution_count":null}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}